# CS778-Foundations-Of-Modern-AI
THis Porject focuses on implementing key RL-Algos like Policy Gradient TRPO and PPO and study them under 5 gym enviroments along with employing a Hybrid Approach to Enhance Human Alignment of LLMs using a combination of Dueling Bandits as well as DPO.

---

# ğŸš€ Online & MixP DPO Implementation using Pythia-410M

### _Optimized for 8GB VRAM setups â€“ A lightweight adaptation of **Samplers-in-Online-DPO**_

---

## ğŸ“Œ Overview

This repository presents a minimal, GPU-friendly implementation of **Online Direct Preference Optimization (DPO)** and **MixP-DPO**, adapted to run on **Pythia-410M (410M parameters)** with **8GB VRAM**.

It is **based on the official implementation and paper**:

> **"The Crucial Role of Samplers in Online Direct Preference Optimization"**  
> *Shi et al., ICLR 2025*

---

## ğŸ§  Key Highlights

- ğŸ” Adapted original framework to **Pythia-410M** instead of 7B models
- ğŸ“ˆ Implemented **Online DPO (2 iterations)** and **MixP DPO (1 iteration)**
- ğŸ¤– Used **DeBERTa-v3 reward model** due to GPU constraints
- ğŸ“Š All training runs logged on **Weights & Biases**
- ğŸ›  Modular runnable scripts for full pipeline: **Generation â†’ Annotation â†’ Training**

---

## ğŸ“‚ Project Structure
```
alignment/
â”œâ”€â”€ data/                          # Generated & annotated data
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ safe_rlhf/
â”‚       â”œâ”€â”€ gen_online_1b.sh       # Online generation
â”‚       â”œâ”€â”€ gen_mixp1.sh           # MixP generation
â”‚       â”œâ”€â”€ dpo_online_1b.sh       # Online DPO training
â”‚       â”œâ”€â”€ dpo_mixp.sh            # MixP DPO training
â”‚       â””â”€â”€ annotate.sh            # Reward annotation
â”œâ”€â”€ generation/
â”‚   â””â”€â”€ safe_rlhf/
â”‚       â”œâ”€â”€ get_hf2.py
â”‚       â””â”€â”€ mixp.py                # MixP dataset merger
â”œâ”€â”€ dpo_iteration/
â”‚   â””â”€â”€ run_dpo.py                 # Core DPO logic
â”œâ”€â”€ configs/                       # Accelerate & training configs
â””â”€â”€ README.md
```

---

## âš™ï¸ Conda Environments Used

| Stage      | Conda Env    | Purpose              |
|------------|--------------|----------------------|
| Generation | `vllm`       | Fast inference       |
| Annotation | `rewardflow` | Reward model scoring |
| Training   | `rlhflow`    | DPO fine-tuning      |

---

## ğŸš€ Usage Pipeline

### ğŸ”¹ 1ï¸âƒ£ Online DPO â€” Example for Iteration 2
```bash
# Generation (vllm env)
bash scripts/safe_rlhf/gen_online_1b.sh 2 3 online

# Annotation (rewardflow env)
bash scripts/safe_rlhf/annotate.sh 2 online 3

# Training (rlhflow env)
wandb login
bash scripts/safe_rlhf/dpo_online_1b.sh 2
```

### ğŸ”¹ 2ï¸âƒ£ MixP DPO â€” Example for Iteration 1
```bash
# Generation (vllm env)
bash scripts/safe_rlhf/gen_mixp1.sh 1 4

# If merge needed
python generation/mixp.py \
  --policy ./data/gen_data0_policy.json \
  --ref ./data/gen_data0_ref.json \
  --output ./data/gen_data_iter1_mixp.json

# Annotation (rewardflow env)
bash scripts/safe_rlhf/annotate.sh 1 mixp 4

# Training (rlhflow env)
wandb login
bash scripts/safe_rlhf/dpo_mixp.sh 1
```

---

## ğŸ“Š Results & Observations

| Method     | Model Size | Iterations | Accuracy Trend            |
|------------|------------|------------|---------------------------|
| Online DPO | 410M       | 2          |  Improved accuracy, unstable       |
| MixP DPO   | 410M       | 1          | Good Mix between accuracy and Offline Stability  |

ğŸ“Œ **Detailed charts available in W&B runs:**
```
online_410m_iter1, online_410m_iter2, mixp_410m_iter1
```

---

## ğŸ“ References

### ğŸ”¸ Paper Citation
```bibtex
@inproceedings{
  shi2024crucialroleosamplerdpo,
  title={The Crucial Role of Samplers in Online Direct Preference Optimization},
  author={Ruizhe Shi and Runlong Zhou and Simon S. Du},
  booktitle={The Thirteenth International Conference on Learning Representations},
  year={2025},
  url={https://openreview.net/forum?id=F63ztufcKw}
}
```

### ğŸ”¸ Original Repository Citation
```bibtex
@software{shi2024samplersgithub,
  author = {Ruizhe Shi and collaborators},
  title = {Samplers-in-Online-DPO},
  year = {2024},
  url = {https://github.com/szze/Samplers-in-Online-DPO},
  note = {Official DPO implementation}
}
```

---

## ğŸ” Future Work

- [ ] Extend MixP to multiple iterations
- [ ] Experiment with LoRA to enable larger models
- [ ] Try high-capacity reward models via Triton or CPU offloading

---

## Contributors
- **Aarsh Kaushik** ([@Aarsh59](https://github.com/Aarsh59))
- **Keyansh Vaish**
- **Tanmya Siddharth** ([@siriuslythough](https://github.com/siriuslythough))


Indian Institute of Technology Kanpur  
ğŸ“¬ Open to collaboration and discussions!

---

## ğŸ§¾ License

MIT License â€“ see [LICENSE](LICENSE) file.

---

## â­ Acknowledgments

If this repo helps you, consider giving the [original authors' repository](https://github.com/szze/Samplers-in-Online-DPO) a â­ on GitHub and citing their work.
